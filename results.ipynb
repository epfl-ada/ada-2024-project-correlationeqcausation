{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T11:24:44.247928Z",
     "start_time": "2024-11-15T11:24:44.245226Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe3fbd",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1927a724623b47",
   "metadata": {},
   "source": [
    "## How datasets are joined\n",
    "\n",
    "### Movie dataset and Character dataset\n",
    "We join the two datasets on the `freebase_movie_id`.\n",
    "\n",
    "### Character dataset and Oscar dataset\n",
    "Oscar dataset does not have `freebase_movie_id` or `freebase_actor_id`. We instead use `parsed_actor_name` and `movie_identifier`. `parsed_actor_name` will be unique for each movie as we drop actors if they share `parsed_actor_name` from playing another character in the same movie. `movie_identifier` is a combination of `parsed_movie_name` and `release_year`. This is unique as we drop movies that share `movie_identifier`.\n",
    "\n",
    "### Resulting dataset from previous steps and IMDb dataset\n",
    "We join these datasets using a combination of `parsed_movie_name` and `release_year` as primary key.\n",
    "<br><br><br>\n",
    "The resulting dataset after the entire pipeline is run is written to `cache/data.csv`, ready for use in P3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309d2c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T11:25:31.018305Z",
     "start_time": "2024-11-15T11:24:44.261753Z"
    }
   },
   "outputs": [],
   "source": [
    "%run data_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a4f09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T11:25:32.927001Z",
     "start_time": "2024-11-15T11:25:31.648108Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv('cache/data.csv', sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f41dcc",
   "metadata": {},
   "source": [
    "## Descriptive statistics and data limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9e9eb",
   "metadata": {},
   "source": [
    "### NaN-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3500cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of NaN values in each column:')\n",
    "movie_df.isnull().sum() * 100 / len(movie_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e28451",
   "metadata": {},
   "source": [
    "Some columns are critical, yet have high share of NaN values, e.g. actor_ethnicity and box_office_revenue \n",
    "\n",
    "We have asked TAs for input on how to handle these values, we see two options:\n",
    "1. Make a fully cleaned dataset with no NaNs\n",
    "2. Have different subsets of data for different analysis questions\n",
    "\n",
    "Two columns are specific to rows that have been Oscar nominated (category, winner). It is therefore no problem that they have many NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab043e0",
   "metadata": {},
   "source": [
    "We examine how much of the data would be lost if we drop all rows with NaN-values in relevant columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7077fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of data points before dropping NaN values: {len(movie_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points_after_drop = len(movie_df.dropna(subset=['title', 'release_date', 'box_office_revenue', 'runtime', 'languages',\n",
    "       'countries', 'genres', 'movie_identifier', 'actor_gender',\n",
    "       'actor_height', 'actor_ethnicity', 'actor_name', 'actor_age',\n",
    "       'parsed_actor_name', 'actor_identifier', 'identifier','year', 'has_rating', 'average_rating',\n",
    "       'number_of_votes']))\n",
    "\n",
    "print(f'Number of complete data points we would have if we dropped all NaN values in relevant columns: {data_points_after_drop}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817428b7",
   "metadata": {},
   "source": [
    "We see that a significant portion of the data (~95%) would be lost by removing rows with relevant NaN-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee0ec6",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['oscar_nominated', 'number_of_votes', 'average_rating', 'actor_height', 'runtime', 'box_office_revenue']\n",
    "numerical_df = movie_df[cols].dropna()\n",
    "print('Nr. of datapoints in the correlation analysis', len(numerical_df))\n",
    "numerical_df.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03800f6",
   "metadata": {},
   "source": [
    "Most entries in the correlation matrix are positive. The ones that are negative are small. \n",
    "\n",
    "Below we analyze the p-value for the correlation to see between which relations it is significant and between which it is not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c6a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating p-values and storing them in the lists 'significant' and 'insignificant' depending on the test outcome. \n",
    "p_values_matrix = []\n",
    "insignificant = []\n",
    "significant = []\n",
    "for col1 in cols: \n",
    "    p_values_list = []\n",
    "    for col2 in cols: \n",
    "        if pearsonr(numerical_df[col1], numerical_df[col2])[1] > (0.05 / 30):  # 95% confidence level adjusted to bonferroni correction \n",
    "            insgnificant.append((col1, col2))\n",
    "        else: \n",
    "            significant.append((col1, col2))\n",
    "            \n",
    "\n",
    "# Printing findings: \n",
    "print(len(significant) - len(cols), 'entries in correlation matrix have significant p-value') # Removing self-correlation\n",
    "print(len(insignificant), 'entries in correlation matrix have insignificant p-value')\n",
    "print()\n",
    "\n",
    "# Printing significant column pairs and skipping self-relations. \n",
    "print('Significant pairs: ')\n",
    "for significant_pair in significant: \n",
    "    if significant_pair[0] != significant_pair[1]:\n",
    "        print(significant_pair[0],'&', significant_pair[1])\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Printing insignificant column pairs\n",
    "print('Insignificant pairs: ')\n",
    "for insignificant_pair in insignificant: \n",
    "    print(insignificant_pair[0], '&', insignificant_pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d573474",
   "metadata": {},
   "source": [
    "The above result indicates that we believe 18 of 30 of the entries in the correlation matrix to be significant at the 95% level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda7859",
   "metadata": {},
   "source": [
    "# Country/nomination analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_character_oscar_rating_df contains a row for each actor/movie pair. We select the non-American actors and compare with the American actors\n",
    "\n",
    "# All actors/movie rows, American and non-American\n",
    "total_actors_num = len(movie_df['countries'])\n",
    "american_total_actors_num = len(movie_df[movie_df['countries'].str.contains('United States of America')])\n",
    "non_american_total_actors_num = total_actors_num - american_total_actors_num\n",
    "\n",
    "# All actors/movie rows with an Oscar nomination, American and non-American\n",
    "total_nominated_actors_num = len(movie_df[movie_df['oscar_nominated'] == True]['countries'])\n",
    "american_nominations_num = len(movie_df[(movie_df['countries'].str.contains('United States of America')) & (movie_df['oscar_nominated'] == True)])\n",
    "non_american_nominations_num = total_nominated_actors_num - american_nominations_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed probability of American actor getting nominated for a film\n",
    "p_american = american_nominations_num / american_total_actors_num\n",
    "p_non_american = non_american_nominations_num / non_american_total_actors_num\n",
    "\n",
    "# We perform a two-sided hypothesis test for whether non-American actors have the same binomial probability of getting nominated as American ones\n",
    "stats.binomtest(non_american_nominations_num, non_american_total_actors_num, p_american)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e369ac",
   "metadata": {},
   "source": [
    "Using alpha=0.05. P-value=2.6118071094409342e-307 < 0.05. We can safely discard the null hypothesis that these have the same probability distribution, and conclude that there is a significantly different probability of being nominated for an Oscar for American and non-American actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecccb059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fraction of American actors nominated for an Oscar:',round(p_american, 5))\n",
    "print('Fraction of non-American actors nominated for an Oscar:', round(p_non_american, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1d261",
   "metadata": {},
   "source": [
    "We see that the observed probability of being nominated is higher for actors in American movies. We believe based on this analysis that the Oscar nominations are generally skewed with higher chances for actors in American movies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb2de70",
   "metadata": {},
   "source": [
    "# Logistic regression on movie and actor traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06834535",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd54a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the most common ethnicities\n",
    "movie_df.groupby('actor_ethnicity').count().sort_values(by='title', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd3d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most frequent ethnicities, in descending order.\n",
    "# Found the mappings manually, by looking the Freebase ethnicity ids up.\n",
    "\n",
    "# An alternate solution would probably be to download a Freebase data dump and join using that.\n",
    "# However, the dataset is quite large so we chose to go this route instead.\n",
    "ethnicity_map = {    \n",
    "    'Indian' : '/m/0dryh9k',\n",
    "    'Black' : '/m/0x67',\n",
    "    'Jewish' : '/m/041rx', \n",
    "    'English' : '/m/02w7gg',\n",
    "    'Irish_Americans' : '/m/033tf_',\n",
    "    'Italian_Americans' : '/m/0xnvg',\n",
    "    'White_people' : '/m/02ctzb',\n",
    "    'White_Americans' : '/m/07hwkr',\n",
    "    'Scottish_Americans': '/m/07bch9',\n",
    "    # '???' : '/m/044038p', Could not find what this Freebase id maps to\n",
    "    'Irish_people' : '/m/03bkbh',\n",
    "    'British' : '/m/0d7wh',\n",
    "    'French' : '/m/03ts0c',\n",
    "    'Italians' : '/m/0222qb',\n",
    "    'Tamil' : '/m/01rv7x',   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5db19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We normalize the data before performing logistic regression\n",
    "def normalize_column(df_column):\n",
    "    return (df_column - df_column.mean()) / df_column.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_movie_df = movie_df.copy(deep=True)\n",
    "features_to_normalize = ['actor_age', 'box_office_revenue', 'runtime', 'actor_height', 'year', 'average_rating', 'number_of_votes',]\n",
    "normalized_movie_df[features_to_normalize] = normalized_movie_df[features_to_normalize].apply(normalize_column)\n",
    "\n",
    "# Encode oscar_nominated as 0 or 1 for logistic regression\n",
    "normalized_movie_df['oscar_nominated'] = normalized_movie_df['oscar_nominated'].astype(int)\n",
    "\n",
    "# One-hot encoding the 5 most frequent ethnicities for the logistic regression:\n",
    "ethnicities = list(ethnicity_map.keys())[:5]\n",
    "for name in ethnicities:\n",
    "    normalized_movie_df[name] = normalized_movie_df['actor_ethnicity'].map(lambda ethnicity: 1 if ethnicity == ethnicity_map[name] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following regression and plotting code was inspired and/or copied from the solution to exercise 4\n",
    "# We perform logistic regression using a selection of relevant features from the dataframe\n",
    "mod = smf.logit(formula='oscar_nominated ~  runtime + box_office_revenue + actor_height + \\\n",
    "                        actor_age + year + average_rating + number_of_votes + \\\n",
    "                        C(Indian) + C(Black) + C(Jewish) + C(English) + C(Irish_Americans)', data=normalized_movie_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f9cd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model and print results\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f560f-ebd6-4ab0-8cc1-66e6401a1969",
   "metadata": {},
   "source": [
    "Note: we get the runtime warning as 1+np.exp(-X) gets so massive it is not computed properly. It does not have impact on the output which will be 0 anyways (division by a very large number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature names\n",
    "variables = res.params.index\n",
    "\n",
    "# quantifying uncertainty!\n",
    "\n",
    "# coefficients\n",
    "coefficients = res.params.values\n",
    "\n",
    "# p-values\n",
    "p_values = res.pvalues\n",
    "\n",
    "# standard errors\n",
    "standard_errors = res.bse.values\n",
    "\n",
    "#confidence intervals\n",
    "res.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort them all by coefficients\n",
    "l1, l2, l3, l4 = zip(*sorted(zip(coefficients[1:], variables[1:], standard_errors[1:], p_values[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe28cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.errorbar(l1, np.array(range(len(l1))), xerr= 2*np.array(l3), linewidth = 1,\n",
    "             linestyle = 'none',marker = 'o',markersize= 3,\n",
    "             markerfacecolor = 'black',markeredgecolor = 'black', capsize= 5)\n",
    "\n",
    "plt.vlines(0,0, len(l1), linestyle = '--')\n",
    "\n",
    "plt.yticks(range(len(l2)),l2)\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Logistic regression coefficient by feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ae323",
   "metadata": {},
   "source": [
    "Lines around the points represent the confidence interval for the coefficient of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d9a616df340add",
   "metadata": {},
   "source": [
    "From this plot we see that there are multiple factors that can be used to predict whether a movie/actor row will be nominated or not. This serves as an initial analysis, we will do this more thoroughly in P3 to make more relevant conclusions for our research questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50e0f3",
   "metadata": {},
   "source": [
    "# Review analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97042b70",
   "metadata": {},
   "source": [
    "To extract movies with nominated actors we need to find every movie where at least one of the rows in the column 'oscar_nominated' is positive.\n",
    "To extract movies without a nominated actor we need to find every movie where every row in the column 'oscar_nominated' is false. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping all movies by title, into unique_movies_df\n",
    "unique_movies_df = movie_df.groupby('movie_identifier').first().reset_index()\n",
    "\n",
    "print('Shape before: ', unique_movies_df.shape)\n",
    "unique_nominated_movies_df = movie_df[movie_df['oscar_nominated'] == True].groupby('movie_identifier').first().reset_index()\n",
    "# Mask is true if a movie from unique_movies_df is not in the dataframe unique_nominated_movies_df\n",
    "mask = unique_movies_df['movie_identifier'].isin(unique_nominated_movies_df['movie_identifier']) == False\n",
    "\n",
    "# Applying the mask \n",
    "not_nominated_df = unique_movies_df[mask]\n",
    "# Checking the intersection between nominated and not nominated movies, should be 0 \n",
    "print('Intersection between nominated and not nominated: ', pd.Series(list(set(unique_nominated_movies_df['movie_identifier']).intersection(set(not_nominated_df['movie_identifier'])))))\n",
    "\n",
    "unique_movies_df = pd.concat([unique_nominated_movies_df, not_nominated_df], axis = 0) \n",
    "print('Shape after: ', unique_movies_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049dbc8",
   "metadata": {},
   "source": [
    "Intersection is [], hence selection worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing movies without imdb ratings\n",
    "movie_unique_with_rating_df = unique_movies_df[unique_movies_df['average_rating'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e85066",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Movies with oscar nominated actors with ratings: ', len(movie_unique_with_rating_df[movie_unique_with_rating_df['oscar_nominated'] == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d87ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting nominated and movies and not nominated movies \n",
    "nominated = movie_unique_with_rating_df[movie_unique_with_rating_df['oscar_nominated']]\n",
    "not_nominated = movie_unique_with_rating_df[movie_unique_with_rating_df['oscar_nominated'] == False]\n",
    "assert nominated.shape[0] + not_nominated.shape[0] == movie_unique_with_rating_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e87775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We exclude all movies with fewer than 30 reviews. There are no movies with oscar nominated actors with fewer than 30 reviews.\n",
    "# This is based on a rule of thumb to exclude outliers / low confidence values \n",
    "excluded = not_nominated[not_nominated['number_of_votes'] < 30]\n",
    "print('Excluded nr of movies from analysis due to few reviews (< 30): ', len(excluded))\n",
    "not_nominated = not_nominated[not_nominated['number_of_votes'] >= 30]\n",
    "nominated = nominated[nominated['number_of_votes'] >= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6376fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical CDF for nominated and not nominated \n",
    "\n",
    "sns.histplot(nominated, x=\"average_rating\", stat = 'density', color = 'gold',label ='Nominated', bins =40)\n",
    "sns.histplot(not_nominated, x=\"average_rating\", stat=\"density\", color = 'grey', label = 'Not nominated', bins = 50)\n",
    "\n",
    "plt.title('Rating distribution of movies')\n",
    "plt.xlabel('IMDB rating')\n",
    "plt.ylabel('Probability density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4c9a3",
   "metadata": {},
   "source": [
    "These empirical distributions look different. We use a two sample Kolmogorov-Smirnov test to test if they are different. The null hypothesis is that the observations come from the same distribution. We reject the null hypothesis if the p-value < 0.05. Also we note that if the test statistic is 0 the distributions are identical and if the test statistic is 1 the distributions are completely different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b8752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T11:26:33.947220162Z",
     "start_time": "2024-11-15T10:30:21.517393Z"
    }
   },
   "outputs": [],
   "source": [
    "stats.kstest(nominated['average_rating'], not_nominated['average_rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e23719",
   "metadata": {},
   "source": [
    "Test statistic is 0.55, meaning distributions are different but not completely different. \n",
    "P-value = 1.6450047550532477e-259. This is extremely small, we can safely reject the null hypothesis. The conclusion is that the distributions are in fact different distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8104702e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T11:26:33.948110156Z",
     "start_time": "2024-11-15T10:30:22.084555Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting reviews per IMDB reviews per movie \n",
    "sns.histplot(not_nominated, x=\"number_of_votes\", bins=50, label = 'Not nominated', color = 'grey')\n",
    "sns.histplot(nominated, x=\"number_of_votes\", bins=50, label = 'Nominated', color = 'gold')\n",
    "plt.yscale('log')\n",
    "plt.title('Review distribution')\n",
    "plt.xlabel('Reviews per movie (millions)')\n",
    "plt.ylabel('Nr. of movies (log)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kstest(not_nominated['number_of_votes'], nominated['number_of_votes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2da46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T11:26:33.948325120Z",
     "start_time": "2024-11-15T10:30:22.552151Z"
    }
   },
   "outputs": [],
   "source": [
    "# As per the plot above, most movies with nominated actors have fewer than 500 000 reviews.\n",
    "# We zoom in and look at the movies with few reviews. \n",
    "\n",
    "lim_not_nominated = not_nominated[not_nominated['number_of_votes'] < 10000]\n",
    "lim_nominated = nominated[nominated['number_of_votes'] < 10000]\n",
    "\n",
    "print('Share of not nominated movies with fewer than 10 000 reviews:', round(len(lim_not_nominated)/len(not_nominated)*100,2), '%')\n",
    "print('Share of nominated with fewer than 10 000 reviews:', round(len(lim_nominated)/(len(nominated))*100,2),  '%')\n",
    "\n",
    "sns.histplot(lim_not_nominated, x=\"number_of_votes\", bins=50, label = 'Not nominated', color = 'grey')\n",
    "sns.histplot(lim_nominated, x=\"number_of_votes\", bins=50, label = 'Nominated', color = 'gold')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.title('Reviews per movie')\n",
    "plt.xlabel('Reviews')\n",
    "plt.ylabel('Nr. of movies (log)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f47d6",
   "metadata": {},
   "source": [
    "We can see that most movies with relatively few review are not nominated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db98b35",
   "metadata": {},
   "source": [
    "## Box-Office Revenue\n",
    "\n",
    "Note: we will inflation adjust box-office revenues in P3 for higher accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4125cd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T11:26:33.948528301Z",
     "start_time": "2024-11-15T10:30:23.647085Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(not_nominated, x=\"box_office_revenue\", stat=\"density\", color = 'grey', label = 'Not nominated', bins = 60)\n",
    "sns.histplot(nominated, x=\"box_office_revenue\", stat = 'density', color = 'gold',label ='Nominated', bins = 40)\n",
    "\n",
    "plt.title('Box office revenue distribution')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Box-office revenue (billions) ')\n",
    "plt.ylabel('Probability density (log)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f74dc",
   "metadata": {},
   "source": [
    "Notice that the above plot is a probability distribution and that the y-axis is in log scale. We are surprised since all movies with nominated actors do not seem to be the ones with the highest revenue. To investigate this we look into movies with lower box-office revenue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5bd63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T11:26:33.948706094Z",
     "start_time": "2024-11-15T10:30:24.598682Z"
    }
   },
   "outputs": [],
   "source": [
    "lim_not_nominated = not_nominated[not_nominated['box_office_revenue'] < 10**7]\n",
    "lim_nominated = nominated[nominated['box_office_revenue'] < 10**7]\n",
    "\n",
    "sns.histplot(lim_not_nominated, x=\"box_office_revenue\", stat=\"density\", color = 'grey', label = 'Not nominated', bins = 20)\n",
    "sns.histplot(lim_nominated, x=\"box_office_revenue\", stat = 'density', color = 'gold',label ='Nominated', bins = 20)\n",
    "\n",
    "plt.title('Box office revenue distribution for movies with revenue less than 10**7')\n",
    "plt.xlabel('Box-office revenue (10s of millions)')\n",
    "plt.ylabel('Probability density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0f513",
   "metadata": {},
   "source": [
    "We can see that movies with nominated actors have revenue in an interval. They are neither the movies with the highest revenue, or the movies with the lowest revenue. We think this will be a hypothesis to explore further in P3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
